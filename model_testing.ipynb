{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from celery import Celery\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from recommender.read_db import ReadDatabase\n",
    "from recommender.models import CollaborativeFiltering\n",
    "from recommender.preprocess import SetUpDataframes\n",
    "from recommender.datasets import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "# environment variables\n",
    "DB_URI = os.environ.get(\"DB_URI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up database reader\n",
    "engine = create_engine(DB_URI)\n",
    "reader_cm = ReadDatabase(engine, 'CM_BD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_data(filename):\n",
    "    question_df = reader_cm.get_data(\n",
    "        'id, post_id',\n",
    "        'posts_question',\n",
    "        None)\n",
    "\n",
    "    taxonomy_df = reader_cm.get_data(\n",
    "        'post_id, area_id',\n",
    "        'posts_taxonomy',\n",
    "        None)\n",
    "\n",
    "    content_df = reader_cm.get_data(\n",
    "        'id, min_range, max_range',\n",
    "        'posts_post',\n",
    "        None)\n",
    "\n",
    "    interaction_df = reader_cm.get_data(\n",
    "        'user_id, post_id',\n",
    "        'posts_interaction',\n",
    "        \"type IN ('sended', 'sent')\")\n",
    "    interaction_df = interaction_df[~interaction_df['post_id'].isna()]\n",
    "    interaction_df['post_id'] = interaction_df['post_id'].astype('int32')\n",
    "\n",
    "    response_df = reader_cm.get_data(\n",
    "        'user_id, response, question_id',\n",
    "        'posts_response',\n",
    "        \"created_at >= '2019-09-20'\",\n",
    "        None)\n",
    "    response_df = response_df[\n",
    "        (response_df['response'].apply(lambda x: x.isdigit())) & (response_df['response'] != '0')]\n",
    "    response_df = response_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    fresh_data = {\n",
    "        'question_df': question_df.to_json(),\n",
    "        'taxonomy_df': taxonomy_df.to_json(),\n",
    "        'content_df': content_df.to_json(),\n",
    "        'interaction_df': interaction_df.to_json(),\n",
    "        'response_df': response_df.to_json()\n",
    "    }\n",
    "\n",
    "    with open(f'{filename}.pkl', 'wb') as f:\n",
    "        pickle.dump(fresh_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id, months, data_required):\n",
    "    model = CollaborativeFiltering()\n",
    "\n",
    "    model.load_model('afinidata_recommender_model_specs')\n",
    "\n",
    "    return model.afinidata_recommend(user_id=user_id, months=months, data_required=data_required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_recommendation_response(user_id, response):\n",
    "    recommendation_df = pd.read_json(recommend(user_id, 10, fresh_data))\n",
    "    post_id = recommendation_df.iloc[0]['post_id']\n",
    "    question_id = recommendation_df.iloc[0]['question_id']\n",
    "    print(recommendation_df.iloc[0])\n",
    "    \n",
    "    new_response = pd.DataFrame([[user_id, response, question_id]], columns=['user_id', 'response', 'question_id'])\n",
    "    response_df = pd.read_json(fresh_data['response_df']).append(new_response, ignore_index=True)\n",
    "    fresh_data['response_df'] = response_df.to_json()\n",
    "\n",
    "    new_interaction = pd.DataFrame([[user_id, post_id]], columns=['user_id', 'post_id'])\n",
    "    interaction_df = pd.read_json(fresh_data['interaction_df']).append(new_interaction, ignore_index=True)\n",
    "    fresh_data['interaction_df'] = interaction_df.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "reading columns id, post_id from table posts_question from database CM_BD\n",
      "----------------------------------------------------------------------\n",
      "reading columns post_id, area_id from table posts_taxonomy from database CM_BD\n",
      "----------------------------------------------------------------------\n",
      "reading columns id, min_range, max_range from table posts_post from database CM_BD\n",
      "----------------------------------------------------------------------\n",
      "reading columns user_id, post_id from table posts_interaction from database CM_BD\n",
      "----------------------------------------------------------------------\n",
      "reading columns user_id, response, question_id from table posts_response from database CM_BD\n"
     ]
    }
   ],
   "source": [
    "refresh_data('afinidata_fresh_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'afinidata_fresh_data.pkl', 'rb') as f:\n",
    "    fresh_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "total number of responses in response_df: 6536\n"
     ]
    }
   ],
   "source": [
    "# extract data from posts_response into a pandas dataframe and\n",
    "# slightly process only relevant data for training\n",
    "# in this case, so far we are only considering data for which\n",
    "# there is an alpha value in the 'response' column\n",
    "response_df = pd.read_json(fresh_data['response_df'])\n",
    "\n",
    "print('*' * 80)\n",
    "print(f'total number of responses in response_df: {len(response_df)}')\n",
    "\n",
    "# create matrix for training with items over rows and users over columns\n",
    "# as a numpy matrix\n",
    "response_matrix = SetUpDataframes.response_matrix(response_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "        3., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_matrix.loc[:,50].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=12000\n",
    "lr=0.00001\n",
    "alpha=0.\n",
    "depth=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "training recommendation model for 12000 epochs with learning rate 1e-05 and \n",
      "hyperparameters regularization: 0.0 / latent features: 2\n",
      "********************************************************************************\n",
      "Epoch 00001 / train loss 4.733221 / test loss 4.759761\n",
      "Epoch 00101 / train loss 0.628564 / test loss 0.583090\n",
      "Epoch 00201 / train loss 0.618718 / test loss 0.574798\n",
      "Epoch 00301 / train loss 0.609623 / test loss 0.567358\n",
      "Epoch 00401 / train loss 0.601189 / test loss 0.560505\n",
      "Epoch 00501 / train loss 0.593351 / test loss 0.554186\n",
      "Epoch 00601 / train loss 0.586056 / test loss 0.548351\n",
      "Epoch 00701 / train loss 0.579253 / test loss 0.542956\n",
      "Epoch 00801 / train loss 0.572896 / test loss 0.537961\n",
      "Epoch 00901 / train loss 0.566947 / test loss 0.533330\n",
      "Epoch 01001 / train loss 0.561367 / test loss 0.529031\n",
      "Epoch 01101 / train loss 0.556125 / test loss 0.525034\n",
      "Epoch 01201 / train loss 0.551191 / test loss 0.521314\n",
      "Epoch 01301 / train loss 0.546538 / test loss 0.517847\n",
      "Epoch 01401 / train loss 0.542142 / test loss 0.514610\n",
      "Epoch 01501 / train loss 0.537981 / test loss 0.511585\n",
      "Epoch 01601 / train loss 0.534036 / test loss 0.508754\n",
      "Epoch 01701 / train loss 0.530289 / test loss 0.506102\n",
      "Epoch 01801 / train loss 0.526724 / test loss 0.503613\n",
      "Epoch 01901 / train loss 0.523326 / test loss 0.501274\n",
      "Epoch 02001 / train loss 0.520082 / test loss 0.499075\n",
      "Epoch 02101 / train loss 0.516980 / test loss 0.497003\n",
      "Epoch 02201 / train loss 0.514010 / test loss 0.495049\n",
      "Epoch 02301 / train loss 0.511161 / test loss 0.493204\n",
      "Epoch 02401 / train loss 0.508424 / test loss 0.491460\n",
      "Epoch 02501 / train loss 0.505791 / test loss 0.489810\n",
      "Epoch 02601 / train loss 0.503255 / test loss 0.488246\n",
      "Epoch 02701 / train loss 0.500809 / test loss 0.486763\n",
      "Epoch 02801 / train loss 0.498446 / test loss 0.485355\n",
      "Epoch 02901 / train loss 0.496161 / test loss 0.484016\n",
      "Epoch 03001 / train loss 0.493949 / test loss 0.482743\n",
      "Epoch 03101 / train loss 0.491805 / test loss 0.481529\n",
      "Epoch 03201 / train loss 0.489725 / test loss 0.480373\n",
      "Epoch 03301 / train loss 0.487704 / test loss 0.479269\n",
      "Epoch 03401 / train loss 0.485739 / test loss 0.478215\n",
      "Epoch 03501 / train loss 0.483827 / test loss 0.477207\n",
      "Epoch 03601 / train loss 0.481965 / test loss 0.476243\n",
      "Epoch 03701 / train loss 0.480149 / test loss 0.475319\n",
      "Epoch 03801 / train loss 0.478376 / test loss 0.474434\n",
      "Epoch 03901 / train loss 0.476646 / test loss 0.473586\n",
      "Epoch 04001 / train loss 0.474955 / test loss 0.472771\n",
      "Epoch 04101 / train loss 0.473301 / test loss 0.471988\n",
      "Epoch 04201 / train loss 0.471682 / test loss 0.471236\n",
      "Epoch 04301 / train loss 0.470097 / test loss 0.470513\n",
      "Epoch 04401 / train loss 0.468544 / test loss 0.469817\n",
      "Epoch 04501 / train loss 0.467021 / test loss 0.469147\n",
      "Epoch 04601 / train loss 0.465527 / test loss 0.468501\n",
      "Epoch 04701 / train loss 0.464061 / test loss 0.467878\n",
      "Epoch 04801 / train loss 0.462621 / test loss 0.467277\n",
      "Epoch 04901 / train loss 0.461206 / test loss 0.466698\n",
      "Epoch 05001 / train loss 0.459815 / test loss 0.466138\n",
      "Epoch 05101 / train loss 0.458447 / test loss 0.465597\n",
      "Epoch 05201 / train loss 0.457101 / test loss 0.465074\n",
      "Epoch 05301 / train loss 0.455777 / test loss 0.464569\n",
      "Epoch 05401 / train loss 0.454473 / test loss 0.464079\n",
      "Epoch 05501 / train loss 0.453188 / test loss 0.463606\n",
      "Epoch 05601 / train loss 0.451922 / test loss 0.463148\n",
      "Epoch 05701 / train loss 0.450675 / test loss 0.462704\n",
      "Epoch 05801 / train loss 0.449445 / test loss 0.462274\n",
      "Epoch 05901 / train loss 0.448232 / test loss 0.461857\n",
      "Epoch 06001 / train loss 0.447035 / test loss 0.461453\n",
      "Epoch 06101 / train loss 0.445854 / test loss 0.461061\n",
      "Epoch 06201 / train loss 0.444688 / test loss 0.460681\n",
      "Epoch 06301 / train loss 0.443536 / test loss 0.460312\n",
      "Epoch 06401 / train loss 0.442399 / test loss 0.459954\n",
      "Epoch 06501 / train loss 0.441276 / test loss 0.459606\n",
      "Epoch 06601 / train loss 0.440166 / test loss 0.459268\n",
      "Epoch 06701 / train loss 0.439070 / test loss 0.458940\n",
      "Epoch 06801 / train loss 0.437985 / test loss 0.458621\n",
      "Epoch 06901 / train loss 0.436913 / test loss 0.458311\n",
      "Epoch 07001 / train loss 0.435853 / test loss 0.458010\n",
      "Epoch 07101 / train loss 0.434804 / test loss 0.457717\n",
      "Epoch 07201 / train loss 0.433766 / test loss 0.457432\n",
      "Epoch 07301 / train loss 0.432740 / test loss 0.457155\n",
      "Epoch 07401 / train loss 0.431724 / test loss 0.456885\n",
      "Epoch 07501 / train loss 0.430718 / test loss 0.456623\n",
      "Epoch 07601 / train loss 0.429722 / test loss 0.456367\n",
      "Epoch 07701 / train loss 0.428735 / test loss 0.456119\n",
      "Epoch 07801 / train loss 0.427759 / test loss 0.455877\n",
      "Epoch 07901 / train loss 0.426791 / test loss 0.455642\n",
      "Epoch 08001 / train loss 0.425832 / test loss 0.455412\n",
      "Epoch 08101 / train loss 0.424882 / test loss 0.455189\n",
      "Epoch 08201 / train loss 0.423941 / test loss 0.454972\n",
      "Epoch 08301 / train loss 0.423007 / test loss 0.454760\n",
      "Epoch 08401 / train loss 0.422082 / test loss 0.454554\n",
      "Epoch 08501 / train loss 0.421164 / test loss 0.454353\n",
      "Epoch 08601 / train loss 0.420254 / test loss 0.454158\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "datasets = Datasets(response_matrix)\n",
    "train_set, test_set = datasets.train_test_split(0.1)\n",
    "\n",
    "# model initialization\n",
    "model = CollaborativeFiltering()\n",
    "model.actors = {\n",
    "    'users': response_matrix.columns.values,\n",
    "    'items': response_matrix.index.values\n",
    "}\n",
    "model.n_items = len(datasets.posts)\n",
    "model.n_users = len(datasets.users)\n",
    "\n",
    "model.train(\n",
    "    train_matrix=train_set,\n",
    "    test_matrix=test_set,\n",
    "    epochs=epochs,\n",
    "    alpha=alpha,\n",
    "    n_features=depth,\n",
    "    lr=lr,\n",
    "    resume=False\n",
    ")\n",
    "\n",
    "print('*' * 80)\n",
    "model.save_model(f'afinidata_recommender_model_specs')\n",
    "print(f'model has been saved to afinidata_recommender_model_specs.pkl in the local directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(recommend(50, 10, fresh_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         predictions  response     score  normalized  probabilities\n",
      "area_id                                                            \n",
      "cogni       2.909244  7.100000  7.100000    1.141661       0.082128\n",
      "motor       3.194701  6.500000  6.500000   -0.698629       0.517270\n",
      "socio       3.431206  6.583333  6.583333   -0.443033       0.400602\n",
      "predictions    3.61716\n",
      "question_id        323\n",
      "post_id            336\n",
      "area_id          socio\n",
      "response             1\n",
      "Name: 182, dtype: object\n"
     ]
    }
   ],
   "source": [
    "register_recommendation_response(50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
